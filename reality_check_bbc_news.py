# -*- coding: utf-8 -*-
"""reality-check-bbc-news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gES7_SiFENkxcXBcsGmcgT8p5rSk8foP
"""

# Import required modules
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# Install necessary libraries
!pip install transformers

# Import required modules
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# Define the RoBERTa-based question-answering pipeline
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", tokenizer="deepset/roberta-base-squad2")

# Continue with the rest of your code...


# Import required modules
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# Function to get BBC News topics
def get_bbc_news_topics():
    url = 'https://www.bbc.com/news'
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        topics = []

        # Find topics under the 'News' section
        news_section = soup.find('a', {'href': '/news'})
        if news_section:
            topic_list = news_section.find_next('ul')
            if topic_list:
                topics = [(index + 1, topic.text, topic['href']) for index, topic in enumerate(topic_list.find_all('a'))]

        return topics
    else:
        print(f"Error: Unable to fetch topics. Status code: {response.status_code}")
        return []

# Function to scrape data with specified classes
# Function to scrape data with specified classes or XPath expression
def scrape_with_classes_or_xpath(soup, classes_or_xpath):
    scraped_data = {}

    for item in classes_or_xpath:
        elements = soup.select(item) if "contains" not in item else soup.find_all(True, class_=item)
        if elements:
            scraped_data[item] = [element.get_text(strip=True) for element in elements]

    return scraped_data

# ...

# Specify your complex XPath expression
complex_xpath_expression = """
    /*[contains(concat( " ", @class, " " ), concat( " ", "gbfmnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dJMMNx", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "fXIgdM", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "crzIlm", " "))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "e1lr2am05", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "exn3ah91", " "))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "gbfmnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "crzIlm", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dQLcvL", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dDrEZj", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kbvxap", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "eRlFvc", " "))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "czRLo", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dJMMNx", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "crzIlm", " " ))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "gbfmnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "crzIlm", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "hgjZnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "cMUkKA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "rvuBc", " " ))] |
        //*[contains(concat( " ", @class, " " ), concat( " ", "hsVTfF", " " )))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "gbfmnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "crzIlm", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jqwZKz", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "hgjZnQ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "cMUkKA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "kTrQIN", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "decnVo", " " ))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "gclMev", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "dEAAFJ", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "tNXLm", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "gBHnNH", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jjGBqh", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "jSYXeA", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "hOBho", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "DOrVl", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "iqoFjT", " " ))]

    //*[contains(concat( " ", @class, " " ), concat( " ", "mJgTp", " " ))] |
    //*[contains(concat( " ", @class, " " ), concat( " ", "eUBbBH", " "))]"""

# Use RoBERTa for question answering on the user's question and scraped data
for item in complex_xpath_expression.split('|'):
    elements = soup.select(item.strip()) if "contains" not in item else soup.find_all(True, class_=item.strip())
    if elements:
        scraped_data[item.strip()] = [element.get_text(strip=True) for element in elements]

# Print scraped data for verification
for class_name, data_list in scraped_data.items():
    print(f"Class: {class_name}")
    for data in data_list:
        print(f"Data: {data}")
    print()



# Function to generate responses using the RoBERTa model for question answering
def generate_response_roberta(question, context):
    answer = qa_pipeline(question=question, context=context)
    return answer['answer']

# Scrape BBC news topics
topics = get_bbc_news_topics()

if not topics:
    print("No topics found. Exiting.")
else:
    # Display available topics to the user
    print("Available topics:")
    for topic in topics:
        print(f"{topic[0]}. {topic[1]} - URL: https://www.bbc.com{topic[2]}")

    # Ask the user to choose a topic
    try:
        user_choice = input("Enter the number corresponding to the desired topic: ")
        selected_topic = topics[int(user_choice) - 1]

        print(f"You selected: {selected_topic[1]} - URL: https://www.bbc.com{selected_topic[2]}")
        topic_url = f'https://www.bbc.com{selected_topic[2]}'

        # Make a request for the topic page
        topic_response = requests.get(topic_url)

        if topic_response.status_code == 200:
            soup = BeautifulSoup(topic_response.text, 'html.parser')

            # Classes to check
            classes_to_check = ["gclMev", "dEAAFJ", "czRLo", "jqwZKz", "kTrQIN", "dJMMNx", "crzIlm"]

            # Scrape elements with specified classes
            scraped_data = scrape_with_classes(soup, classes_to_check)

            # Ask the user to enter a question for RoBERTa-based question answering
            user_question = input("Enter a question about the selected topic: ")

            # Use RoBERTa for question answering on the user's question and scraped data
            for class_name, data_list in scraped_data.items():
                for data in data_list:
                    question = f"What is {data} about?"
                    answer = generate_response_roberta(question, user_question)
                    print(f"Question: {question}")
                    print(f"Answer: {answer}")
                    print()

        else:
            print(f"Error: Unable to fetch topic page. Status code: {topic_response.status_code}")

    except (ValueError, IndexError):
        print("Invalid choice. Please enter a valid topic number.")


# ...

# Function to check if the news is real by verifying against BBC Reality Check page
def check_reality(news_title):
    reality_check_url = 'https://www.bbc.com/news/reality_check'
    reality_check_response = requests.get(reality_check_url)

    if reality_check_response.status_code == 200:
        reality_check_soup = BeautifulSoup(reality_check_response.text, 'html.parser')
        reality_check_articles = reality_check_soup.find_all('article', class_='gs-c-promo')

        for article in reality_check_articles:
            article_title = article.find('h3', class_='gs-c-promo-heading__title')
            article_paragraphs = article.find_all('p', class_='gs-c-promo-summary')

            if article_title and article_paragraphs:
                title_text = article_title.get_text().lower()
                paragraphs_text = [p.get_text().lower() for p in article_paragraphs]

                if all(keyword.lower() in title_text or keyword.lower() in paragraphs_text for keyword in news_title.split()):
                    # News found on the Reality Check page, considered real
                    article_link = article.find('a', class_='gs-c-promo-heading')['href']
                    full_link = f"https://www.bbc.com{article_link}"
                    return full_link

        return None  # News not found on the Reality Check page
    else:
        print(f"Error: Unable to fetch Reality Check page. Status code: {reality_check_response.status_code}")
        return None

# ...

# Use RoBERTa for question answering on the user's question and scraped data
for class_name, data_list in scraped_data.items():
    for data in data_list:
        question = f"What is {data} about?"
        answer = generate_response_roberta(question, user_question)
        print(f"Question: {question}")
        print(f"Answer: {answer}")

        # Check if the news is real and get the link if it is
        news_title = f"{data} {answer}"
        real_news_link = check_reality(news_title)

        if real_news_link:
            print(f"This news is considered real. Related link: {real_news_link}")
        else:
            print("This news may be fake or is not on the Reality Check page.")

        print()

# Import required modules
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# Install necessary libraries
#!pip install transformers

# Import required modules
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

# Define the RoBERTa-based question-answering pipeline
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", tokenizer="deepset/roberta-base-squad2")

# Function to get BBC News topics
def get_bbc_news_topics():
    url = 'https://www.bbc.com/news'
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        topics = []

        # Find topics under the 'News' section
        news_section = soup.find('a', {'href': '/news'})
        if news_section:
            topic_list = news_section.find_next('ul')
            if topic_list:
                topics = [(index + 1, topic.text, topic['href']) for index, topic in enumerate(topic_list.find_all('a'))]

        return topics
    else:
        print(f"Error: Unable to fetch topics. Status code: {response.status_code}")
        return []

# Function to scrape data with specified classes or XPath expression
def scrape_with_classes_or_xpath(soup, classes_or_xpath):
    scraped_data = {}

    for item in classes_or_xpath:
        elements = soup.select(item) if "contains" not in item else soup.find_all(True, class_=item)
        if elements:
            scraped_data[item] = [element.get_text(strip=True) for element in elements]

    return scraped_data

# Function to generate responses using the RoBERTa model for question answering
def generate_response_roberta(question, context):
    answer = qa_pipeline(question=question, context=context)
    return answer['answer']

# Function to check if the news is real by verifying against BBC Reality Check page
def check_reality(news_title):
    reality_check_url = 'https://www.bbc.com/news/reality_check'
    reality_check_response = requests.get(reality_check_url)

    if reality_check_response.status_code == 200:
        reality_check_soup = BeautifulSoup(reality_check_response.text, 'html.parser')
        reality_check_articles = reality_check_soup.find_all('article', class_='gs-c-promo')

        for article in reality_check_articles:
            article_title = article.find('h3', class_='gs-c-promo-heading__title')
            article_paragraphs = article.find_all('p', class_='gs-c-promo-summary')

            if article_title and article_paragraphs:
                title_text = article_title.get_text().lower()
                paragraphs_text = [p.get_text().lower() for p in article_paragraphs]

                if all(keyword.lower() in title_text or keyword.lower() in paragraphs_text for keyword in news_title.split()):
                    # News found on the Reality Check page, considered real
                    article_link = article.find('a', class_='gs-c-promo-heading')['href']
                    full_link = f"https://www.bbc.com{article_link}"
                    return full_link

        return None  # News not found on the Reality Check page
    else:
        print(f"Error: Unable to fetch Reality Check page. Status code: {reality_check_response.status_code}")
        return None

# Continue with the rest of your code...

# Function to check if the news is real by verifying against BBC Reality Check page
def check_reality(news_title):
    reality_check_url = 'https://www.bbc.com/news/reality_check'
    reality_check_response = requests.get(reality_check_url)

    if reality_check_response.status_code == 200:
        reality_check_soup = BeautifulSoup(reality_check_response.text, 'html.parser')
        reality_check_articles = reality_check_soup.find_all('article', class_='gs-c-promo')

        for article in reality_check_articles:
            article_title = article.find('h3', class_='gs-c-promo-heading__title')
            article_paragraphs = article.find_all('p', class_='gs-c-promo-summary')

            if article_title and article_paragraphs:
                title_text = article_title.get_text().lower()
                paragraphs_text = [p.get_text().lower() for p in article_paragraphs]

                if all(keyword.lower() in title_text or keyword.lower() in paragraphs_text for keyword in news_title.split()):
                    # News found on the Reality Check page, considered real
                    article_link = article.find('a', class_='gs-c-promo-heading')['href']
                    full_link = f"https://www.bbc.com{article_link}"
                    return full_link

        return None  # News not found on the Reality Check page
    else:
        print(f"Error: Unable to fetch Reality Check page. Status code: {reality_check_response.status_code}")
        return None

# ...

# Use RoBERTa for question answering on the user's question and scraped data
for class_name, data_list in scraped_data.items():
    for data in data_list:
        question = f"What is {data} about?"
        answer = generate_response_roberta(question, user_question)
        print(f"Question: {question}")
        print(f"Answer: {answer}")

        # Check if the news is real and get the link if it is
        news_title = f"{data} {answer}"
        real_news_link = check_reality(news_title)

        if real_news_link:
            print(f"This news is considered real. Related link: {real_news_link}")
        else:
            print("This news may be fake or is not on the Reality Check page.")

        print()